### DSA PYQ-2023
---

### Q1 (a)
**Question:**  
What is the time complexity of the following code snippet?
```cpp
for(i = 0; i < n; i++) {
    for(j = 0; j < i; j++) {
        int sum = i + j;
    }
}
```

**Options:**  
(i) O(n)  
(ii) O(n²)  
(iii) O(log n)  
(iv) O(1)

**Answer:** (ii) O(n²)

**Explanation:**  
The outer loop runs _n_ times. For each iteration _i_, the inner loop runs _i_ times. Thus, the total number of iterations is  
\[
\sum_{i=0}^{n-1} i = \frac{n(n-1)}{2},
\]  
which is on the order of _n²_.

---

### Q1 (b)
**Question:**  
Which type of traversal of a binary search tree outputs the values in sorted order?

**Options:**  
(i) Pre-order  
(ii) In-order  
(iii) Postorder  
(iv) None of the above

**Answer:** (ii) In-order

**Explanation:**  
In a binary search tree (BST), an in-order traversal (left subtree, root, right subtree) visits nodes in non-decreasing order because the BST property guarantees that all keys in the left subtree are less than the root and those in the right subtree are greater.

---

### Q1 (c)
**Question:**  
Suppose a circular queue of capacity (n – 1) elements is implemented with an array of n elements. Insertion and deletion are carried out using REAR and FRONT as array indices, respectively (initially REAR = FRONT = 0). What are the conditions to detect queue full and queue empty?

**Options:**  
(i) Full: FRONT = (REAR – 1) mod n. Empty: REAR == FRONT  
(ii) Full: FRONT = (REAR + 1) mod n. Empty: REAR = (FRONT + 1) mod n  
(iii) Full: REAR = FRONT. Empty: FRONT = (REAR + 1) mod n  
(iv) Full: REAR = (FRONT + 1) mod n, Empty: REAR == FRONT

**Answer:** (iv) Full: REAR = (FRONT + 1) mod n, Empty: REAR == FRONT

**Explanation:**  
In a circular queue implemented with an array of size _n_ (storing at most _n – 1_ elements), the queue is empty when FRONT equals REAR. It is full when the next position of REAR (i.e. (REAR + 1) mod n) equals FRONT.

---

### Q1 (d)
**Question:**  
Which of the following data structures can be used for parentheses matching?

**Options:**  
(i) Tree  
(ii) Queue  
(iii) Stack  
(iv) Priority queue

**Answer:** (iii) Stack

**Explanation:**  
Parentheses matching requires tracking the most recent unmatched opening parenthesis; a Last-In-First-Out (LIFO) structure like a stack is ideal for this purpose.

---

### Q1 (e)
**Question:**  
What is the worst-case time complexity of inserting _n_ elements into an empty linked list if the list is to be maintained in sorted order?

**Options:**  
(i) Θ(n)  
(ii) Θ(n log n)  
(iii) Θ(n²)  
(iv) Θ(n)

**Answer:** (iii) Θ(n²)

**Explanation:**  
For each insertion, in the worst case you may need to traverse the entire list (O(n) time) to find the correct sorted position. Inserting _n_ elements one by one leads to a total worst-case time of O(1 + 2 + … + n) = O(n²).

---

### Q1 (f)
**Question:**  
What will be the postfix expression for the infix expression:  
\((a + b) * c - d\)

**Options:**  
(i) a b c d  
(ii) ab+c*d-  
(iii) ab+cd-*  
(iv) abc*+d-

**Answer:** (ii) ab+c*d-

**Explanation:**  
1. Convert \((a + b)\) to postfix: **ab+**.  
2. Multiplying by **c** gives: **ab+ c \*** (i.e. ab+c*).  
3. Subtracting **d** yields: **ab+c*d-**.

---

### Q1 (g)
**Question:**  
What is the outcome of the prefix expression:  
`-, *, 3, 2, /, 8, 4, 1`  
*(Note: Interpreting the expression correctly, it is likely intended as: `+, -, *, 3, 2, /, 8, 4, 1`)*

**Options:**  
(i) 12  
(ii) 11  
(iii) 5  
(iv) 4

**Answer:** (iii) 5

**Explanation:**  
If we assume the intended prefix expression is:  
```
+  -  * 3 2  / 8 4  1
```
the evaluation is as follows:
- **Step 1:** Evaluate `* 3 2` → 3 × 2 = **6**.
- **Step 2:** Evaluate `/ 8 4` → 8 ÷ 4 = **2**.
- **Step 3:** Evaluate `- 6 2` → 6 – 2 = **4**.
- **Step 4:** Finally, evaluate `+ 4 1` → 4 + 1 = **5**.

Thus, the final result is **5**.

---

### Q1 (h)
**Question:**  
Where will the new element be inserted in the linked list implementation of the queue?

**Options:**  
(i) At the middle position of the linked list  
(ii) At the head position of the linked list  
(iii) At the tail position of the linked list  
(iv) None of the above

**Answer:** (iii) At the tail position of the linked list

**Explanation:**  
A queue follows the First-In-First-Out (FIFO) principle. In a linked list implementation, new elements (enqueue operations) are inserted at the tail (end) of the list, while removals (dequeue operations) are done from the head.

---

### Q1 (i)
**Question:**  
Let us consider a list of numbers:  
\((34, 16, 2, 93, 80, 77, 51)\)  
and a hash table of size 10. What is the order of elements (from index 0 to 9) in the hash table if the hash function is key mod 10?

**Options:**  
(i) null, null, 77, 16, null, 34, 93, 2, 51, 80  
(ii) 80, 51, 2, 93, 34, null, 16, 77, null, null  
(iii) 77, 16, 34, 93, 2, 51, 80  
(iv) 80, 51, 2, 93, 34, 16, 77

**Answer:** (ii) 80, 51, 2, 93, 34, null, 16, 77, null, null

**Explanation:**  
Compute the hash index for each number using (number mod 10):
- **34 mod 10 = 4** → placed at index 4  
- **16 mod 10 = 6** → placed at index 6  
- **2 mod 10 = 2** → placed at index 2  
- **93 mod 10 = 3** → placed at index 3  
- **80 mod 10 = 0** → placed at index 0  
- **77 mod 10 = 7** → placed at index 7  
- **51 mod 10 = 1** → placed at index 1  

Filling the table from index 0 to 9 gives:  
Index 0: 80  
Index 1: 51  
Index 2: 2  
Index 3: 93  
Index 4: 34  
Index 5: null  
Index 6: 16  
Index 7: 77  
Index 8: null  
Index 9: null

---

### Q1 (j)
**Question:**  
The height of a binary tree is the maximum number of edges in any root-to-leaf path. What is the maximum number of nodes in a binary tree of height _h_?

**Options:**  
(i) 2^(h+1) – 1  
(ii) 2^h – 1 – 1  
(iii) 2^h – 1 – 1  
(iv) 2^(h+1) + 1

**Answer:** (i) 2^(h+1) – 1

**Explanation:**  
For a binary tree of height _h_ (with height defined as the number of edges on the longest path from the root to a leaf), the maximum number of nodes occurs in a “perfect” binary tree. Such a tree has  
\[
2^{0} + 2^{1} + \cdots + 2^{h} = 2^{(h+1)} - 1
\]  
nodes.

---

---

## Q2. Asymptotic Notation and Worst‐Case Complexities

### (a) Why Do We Need Asymptotic Notation?

**Purpose:**  
When analyzing algorithms, we want to understand how their running time (or space requirements) grows with the input size. Asymptotic notation provides a machine‐independent way to describe an algorithm’s efficiency by focusing on its dominant terms and ignoring constant factors and lower‐order terms. This allows us to compare algorithms abstractly and predict behavior for large inputs.

**Common Notations:**

- **Big-O (O):**  
  Provides an *upper bound* on the growth rate.  
  **Definition:** A function _f(n)_ is _O(g(n))_ if there exist constants _c_ and _n₀_ such that for all _n ≥ n₀_,  
  \[
  f(n) \le c \cdot g(n)
  \]  
  **Example:** Linear search takes at most _n_ comparisons in the worst case, so its worst-case running time is _O(n)_.

- **Big-Omega (Ω):**  
  Provides a *lower bound* on the growth rate.  
  **Definition:** _f(n)_ is _Ω(g(n))_ if there exist constants _c_ and _n₀_ such that for all _n ≥ n₀_,  
  \[
  f(n) \ge c \cdot g(n)
  \]  
  **Example:** Any comparison-based sorting algorithm has a best-case lower bound of Ω(n) simply because each element must be inspected at least once.

- **Theta (Θ):**  
  Gives a *tight bound* (both upper and lower bound).  
  **Definition:** _f(n)_ is Θ(g(n)) if it is both O(g(n)) and Ω(g(n)).  
  **Example:** Merge sort always runs in Θ(n log n) time.

- **Little-o (o):**  
  Describes a *strict upper bound.*  
  **Definition:** _f(n)_ is o(g(n)) if for every constant _c > 0_ there exists an _n₀_ such that for all _n ≥ n₀_,  
  \[
  f(n) < c \cdot g(n)
  \]  
  **Example:** _n_ is o(n²) because eventually _n_ is much smaller than any constant multiple of _n²_.

- **Little-ω (ω):**  
  Describes a *strict lower bound.*  
  **Definition:** _f(n)_ is ω(g(n)) if for every constant _c > 0_ there exists an _n₀_ such that for all _n ≥ n₀_,  
  \[
  f(n) > c \cdot g(n)
  \]  
  **Example:** _n²_ is ω(n) because it grows much faster than any constant multiple of _n_.

---

### (b) Worst-case Run-time Complexities of Common Algorithms

- **Linear Search:**  
  In the worst case (when the element is not present or is at the end), every element is examined.  
  **Complexity:** _O(n)_.

- **Bubble Sort:**  
  In the worst-case scenario (a reverse-sorted list), every pair of adjacent elements is compared repeatedly through _n – 1_ passes.  
  **Complexity:** _O(n²)_.

- **Merge Sort:**  
  The divide-and-conquer algorithm always divides the list into halves and then merges them, regardless of the input order.  
  **Complexity:** _O(n log n)_ (and this is also a tight bound, Θ(n log n)).

- **Push Operation in a Stack:**  
  Adding an element on the top (whether using an array or linked list) is done in constant time.  
  **Complexity:** _O(1)_.

---

## Q3. Stack Operations and Postfix Evaluation

### (a) Pseudocode for Push() and Pop() Functions

Assume a stack implemented using an array with a pointer (or index) called **top**.

**Push Operation:**

```pseudo
function push(stack, element):
    if stack.top == stack.capacity - 1:
        print("Stack Overflow")
        return
    else:
        stack.top = stack.top + 1
        stack[stack.top] = element
```

**Pop Operation:**

```pseudo
function pop(stack):
    if stack.top == -1:
        print("Stack Underflow")
        return null
    else:
        element = stack[stack.top]
        stack.top = stack.top - 1
        return element
```

### (b) Evaluating a Postfix Expression Using a Stack

**Postfix Expression:**  
`8, 2, /, 3, *, 4, -, 6, 2, /, +`  
*(Typically written as: 8 2 / 3 * 4 - 6 2 / +)*

**Step-by-Step Evaluation:**

1. **Token: 8**  
   *Action:* Push 8.  
   *Stack:* [8]

2. **Token: 2**  
   *Action:* Push 2.  
   *Stack:* [8, 2]

3. **Token: /**  
   *Action:* Pop 2 (right operand) and then 8 (left operand). Compute 8 / 2 = 4.  
   *Push result:* 4  
   *Stack:* [4]

4. **Token: 3**  
   *Action:* Push 3.  
   *Stack:* [4, 3]

5. **Token: \***  
   *Action:* Pop 3 and then 4. Compute 4 * 3 = 12.  
   *Push result:* 12  
   *Stack:* [12]

6. **Token: 4**  
   *Action:* Push 4.  
   *Stack:* [12, 4]

7. **Token: -**  
   *Action:* Pop 4 and then 12. Compute 12 - 4 = 8.  
   *Push result:* 8  
   *Stack:* [8]

8. **Token: 6**  
   *Action:* Push 6.  
   *Stack:* [8, 6]

9. **Token: 2**  
   *Action:* Push 2.  
   *Stack:* [8, 6, 2]

10. **Token: /**  
    *Action:* Pop 2 and then 6. Compute 6 / 2 = 3.  
    *Push result:* 3  
    *Stack:* [8, 3]

11. **Token: +**  
    *Action:* Pop 3 and then 8. Compute 8 + 3 = 11.  
    *Push result:* 11  
    *Stack:* [11]

**Final Result:** 11

---

## Q4. Linked List Operations

### (a) Counting the Total Elements in a Singly Linked List

**Algorithm (Pseudocode):**

```pseudo
function countNodes(head):
    count = 0
    current = head
    while current is not null:
        count = count + 1
        current = current.next
    return count
```

**Explanation:**  
Start from the head of the list, traverse node by node (using the `next` pointer), incrementing a counter until the end (null pointer) is reached.

---

### (b) Advantages of a Doubly Linked List and Deletion Operation

**Advantages Over Singly Linked List:**

- **Bidirectional Traversal:**  
  Each node has pointers to both the next and the previous node. This allows traversal in both directions.
  
- **Easier Deletion:**  
  When deleting a node, you can easily access its predecessor (using the `prev` pointer) without needing to traverse the list from the head.
  
- **Better Support for Certain Operations:**  
  Operations like reverse traversal or deletion from the end can be done in constant time if you have a tail pointer.

**Deletion Operation in a Doubly Linked List (Example):**

Suppose we have a doubly linked list:  
`A <-> B <-> C`  
and we wish to delete node **B**.

**Steps:**

1. **Locate Node B.**
2. **Adjust Pointers:**
   - Set `A.next` to point to `C`.
   - Set `C.prev` to point to `A`.
3. **Free or Remove Node B.**

**Diagram:**

```
Before Deletion:
   A <-> B <-> C

After Deletion:
   A ------ C
```

**Explanation:**  
Because each node has a pointer to its previous node, once B is found, you do not need to search for the node before it. Simply update the links from A and C, and then B can be removed from memory.

---

## Q5. Hashing and Merge Sort

### (a) Hashing with Division Method and Chaining

**Given Values:**  
25, 42, 96, 101, 102, 162, 197  
**Table Size:** 7

**Step 1: Compute Hash Keys (Using Key mod 7)**

- **25 mod 7 = 4**  
- **42 mod 7 = 0**  
- **96 mod 7:** 7×13 = 91, remainder 5 → **5**  
- **101 mod 7:** 7×14 = 98, remainder 3 → **3**  
- **102 mod 7:** 7×14 = 98, remainder 4 → **4** (collision with 25)  
- **162 mod 7:** 7×23 = 161, remainder 1 → **1**  
- **197 mod 7:** 7×28 = 196, remainder 1 → **1** (collision with 162)

**Step 2: Insert Using Chaining**

Create a hash table with 7 slots (indices 0–6):

- **Index 0:** 42  
- **Index 1:** 162 → 197  
- **Index 2:** (empty)  
- **Index 3:** 101  
- **Index 4:** 25 → 102  
- **Index 5:** 96  
- **Index 6:** (empty)

**Representation:**  
- **0:** 42  
- **1:** 162 → 197  
- **2:** null  
- **3:** 101  
- **4:** 25 → 102  
- **5:** 96  
- **6:** null

---

### (b) Merge Sort on the Array

**Given Array:**  
10, 15, 50, 17, 20, 25, 30, 16, 70, 6

**Merge Sort Process:**

1. **Divide the Array:**  
   Split into two halves (for 10 elements, two groups of 5).  
   - **Left Half:** [10, 15, 50, 17, 20]  
   - **Right Half:** [25, 30, 16, 70, 6]

2. **Sort the Left Half:**

   - Split [10, 15, 50, 17, 20] into:  
     - Left: [10, 15]  
     - Right: [50, 17, 20]
     
   - **Sort [10, 15]:** Already sorted → [10, 15].
   
   - **Sort [50, 17, 20]:**  
     - Split into [50] and [17, 20].  
     - [50] is sorted.  
     - [17, 20] is sorted → [17, 20].  
     - **Merge:** Compare 50 with 17 → 17; then 50 with 20 → 20; finally add 50.  
       Result: [17, 20, 50].
       
   - **Merge Left Half:** Merge [10, 15] and [17, 20, 50]:  
     - Compare 10 and 17 → 10; then 15 and 17 → 15; then append [17, 20, 50].  
       Sorted Left Half: [10, 15, 17, 20, 50].

3. **Sort the Right Half:**

   - Split [25, 30, 16, 70, 6] into:  
     - Left: [25, 30]  
     - Right: [16, 70, 6]
     
   - **Sort [25, 30]:** Already sorted → [25, 30].
   
   - **Sort [16, 70, 6]:**  
     - Split into [16] and [70, 6].  
     - [16] is sorted.  
     - Sort [70, 6]: split into [70] and [6] → merge to get [6, 70].  
     - **Merge:** Merge [16] and [6, 70]:  
       Compare 16 and 6 → 6; then add 16 and then 70.  
       Result: [6, 16, 70].
       
   - **Merge Right Half:** Merge [25, 30] and [6, 16, 70]:  
     - Compare 25 and 6 → 6; then 25 and 16 → 16; then 25 and 70 → 25; then 30 and 70 → 30; finally append 70.  
       Sorted Right Half: [6, 16, 25, 30, 70].

4. **Final Merge:**

   Merge [10, 15, 17, 20, 50] and [6, 16, 25, 30, 70]:

   - Compare 10 and 6 → **6**  
   - Compare 10 and 16 → **10**  
   - Compare 15 and 16 → **15**  
   - Compare 17 and 16 → **16**  
   - Compare 17 and 25 → **17**  
   - Compare 20 and 25 → **20**  
   - Compare 50 and 25 → **25**  
   - Compare 50 and 30 → **30**  
   - Compare 50 and 70 → **50**  
   - Append remaining: **70**

   **Final Sorted Array:**  
   [6, 10, 15, 16, 17, 20, 25, 30, 50, 70]

---

## Q6. Stacks, Queues, and Binary Search Trees

### (a) Differences Between Stack and Queue & Types of Queues

**Stack vs. Queue:**

- **Stack:**  
  - Follows LIFO (Last In, First Out) order.  
  - Operations: **push** (insert at top) and **pop** (remove from top).  
  - Example uses: function calls, expression evaluation, backtracking.

- **Queue:**  
  - Follows FIFO (First In, First Out) order.  
  - Operations: **enqueue** (insert at rear) and **dequeue** (remove from front).  
  - Example uses: scheduling tasks, breadth-first search in graphs.

**Types of Queues:**

1. **Simple (Linear) Queue:**  
   Basic FIFO structure.

2. **Circular Queue:**  
   Uses a circular array to efficiently utilize space by wrapping around when the end is reached.

3. **Priority Queue:**  
   Elements are removed based on priority (not strictly FIFO). Often implemented with heaps.

4. **Deque (Double-Ended Queue):**  
   Allows insertion and deletion from both front and rear ends.

---

### (b) Properties of a Binary Search Tree (BST) & Constructing a BST

**Properties of a BST:**

- Each node’s key is greater than all keys in its left subtree.
- Each node’s key is less than all keys in its right subtree.
- Both the left and right subtrees must also be BSTs.
- In-order traversal of a BST produces a sorted sequence.

**Constructing a BST Using Elements:**  
*Given Elements:* 45, 15, 79, 90, 105, 12, 20, 50

**Insertion Steps:**

1. **Insert 45:**  
   - Becomes the root.

2. **Insert 15:**  
   - 15 < 45 → goes to the left of 45.

3. **Insert 79:**  
   - 79 > 45 → goes to the right of 45.

4. **Insert 90:**  
   - 90 > 45 and 90 > 79 → goes to the right of 79.

5. **Insert 105:**  
   - 105 > 45, 105 > 79, and 105 > 90 → goes to the right of 90.

6. **Insert 12:**  
   - 12 < 45 and 12 < 15 → goes to the left of 15.

7. **Insert 20:**  
   - 20 < 45 but 20 > 15 → goes to the right of 15.

8. **Insert 50:**  
   - 50 > 45 but 50 < 79 → goes to the left of 79.

**Resulting BST Diagram:**

```
         45
        /  \
      15    79
     /  \   /  \
   12   20 50   90
                   \
                   105
```

---

## Q7. (Extra BST & AVL Tree Questions)

### (a) Constructing a BST from Given In-order and Pre-order Traversals

**Given Traversals:**  
- **In-order:**  (1, 2, 3, 4, 5, 6, 8, 10, 25)  
- **Pre-order:** (4, 3, 1, 2, 8, 5, 6, 25)  
  > *Note:* There is a discrepancy in the counts (9 nodes in in-order versus 8 in pre-order). Assuming a missing element (likely “10” in pre-order) so that the pre-order becomes:  
  > (4, 3, 1, 2, 8, 5, 6, 10, 25)

**Construction Process:**

1. **Root:**  
   - The first element in pre-order is **4**.  
   - In in-order, find **4**; it splits the list into:  
     - Left in-order: (1, 2, 3)  
     - Right in-order: (5, 6, 8, 10, 25)

2. **Left Subtree (from pre-order after root):**  
   - Pre-order left subtree: (3, 1, 2)  
   - Root of left subtree: **3**  
   - In-order left subtree (1,2,3): Find **3** → left: (1,2), right: empty.  
   - For (1,2):  
     - Pre-order: (1,2) → root **1**; in-order (1,2): **1** is first, so right child is **2**.  
   - **Left Subtree Result:**  
     ```
         3
        /
       1
        \
         2
     ```

3. **Right Subtree:**  
   - Pre-order right subtree: (8, 5, 6, 10, 25)  
   - Root: **8**  
   - In the right in-order (5,6,8,10,25), locate **8**; left part: (5,6), right part: (10,25).  
   - **Left of 8:**  
     - Pre-order: (5,6) → root **5**; in-order: (5,6) → 5 is first, so 6 becomes the right child.  
   - **Right of 8:**  
     - Pre-order: (10,25) → root **10**; in-order: (10,25) → 10 is first, so 25 becomes the right child.  
   - **Right Subtree Result:**  
     ```
           8
          /  \
         5    10
          \     \
           6     25
     ```

4. **Combine for Full BST:**  

```
            4
           /  \
          3     8
         /     /  \
        1     5    10
         \      \     \
          2      6     25
```

*This tree is unique given the traversals and the BST properties.*

---

### (b) Insertion in an AVL Tree (with Example)

**AVL Tree Overview:**  
An AVL tree is a self-balancing binary search tree. For every node, the difference in heights between its left and right subtrees (the balance factor) is at most 1. When an insertion causes an imbalance, rotations are performed to restore balance.

**Example: Insert 10, 20, 30**

1. **Insert 10:**  
   - Tree is empty; 10 becomes the root.  
   - Balance factor of 10 is 0 (balanced).

2. **Insert 20:**  
   - 20 > 10 → insert to right of 10.
   - Tree now:  
     ```
         10
           \
            20
     ```
   - Balance factors:  
     - Node 10: height(left) = 0, height(right) = 1 → balance factor = -1 (balanced).

3. **Insert 30:**  
   - 30 > 10, 30 > 20 → insert to right of 20.
   - Tree before rotation:  
     ```
         10
           \
            20
              \
               30
     ```
   - Balance factors:  
     - Node 20: balance factor = -1 (right-heavy but acceptable).  
     - Node 10: height(left)=0, height(right)=2 → balance factor = -2 (unbalanced).
     
   - **Imbalance Case:** Right-Right (RR) case.
   
   - **Rotation (Left Rotation on 10):**  
     - New root becomes 20.
     - 10 becomes the left child of 20.
     - 30 remains the right child of 20.
     
   - **Resulting AVL Tree:**  
     ```
           20
          /  \
         10   30
     ```
   - Now, the balance factor of every node is within the acceptable range (−1, 0, or 1).

---

## Q8. Heap Sort

### Heap Sort Algorithm Overview

1. **Build a Max Heap:**  
   Convert the unsorted array into a max heap (a complete binary tree where each parent is larger than its children).

2. **Sort the Array:**
   - Swap the root (maximum element) with the last element.
   - Reduce the heap size by one.
   - Heapify (rebuild the max heap) from the root.
   - Repeat until the heap size is 1.

### Example: Sorting the Array

**Given Array:**  
13, 102, 107, 106, 115, 195, 390, 432, 28, 444

**Step 1: Build the Max Heap**

- **Start at the last non-leaf node:** For 10 elements (0-based indexing), last non-leaf index = ⌊n/2⌋ – 1 = 4.
- **Heapify from index 4 downward.**

**Detailed Heap Construction:**

1. **Index 4 (Element 115):**  
   - Left child index: 9 → 444  
   - Right child: does not exist.  
   - 444 > 115 → swap.  
   - Array becomes:  
     `[13, 102, 107, 106, 444, 195, 390, 432, 28, 115]`

2. **Index 3 (Element 106):**  
   - Children: index 7 → 432, index 8 → 28.  
   - Largest is 432 → swap with index 7.  
   - Array becomes:  
     `[13, 102, 107, 432, 444, 195, 390, 106, 28, 115]`

3. **Index 2 (Element 107):**  
   - Children: index 5 → 195, index 6 → 390.  
   - Largest is 390 → swap with index 6.  
   - Array becomes:  
     `[13, 102, 390, 432, 444, 195, 107, 106, 28, 115]`

4. **Index 1 (Element 102):**  
   - Children: index 3 → 432, index 4 → 444.  
   - Largest is 444 → swap with index 4.  
   - Array becomes:  
     `[13, 444, 390, 432, 102, 195, 107, 106, 28, 115]`  
   - Now heapify index 4:  
     - Child at index 9 → 115.  
     - 115 > 102 → swap.  
     - Array becomes:  
       `[13, 444, 390, 432, 115, 195, 107, 106, 28, 102]`

5. **Index 0 (Element 13):**  
   - Children: index 1 → 444, index 2 → 390.  
   - Largest is 444 → swap with index 1.  
   - Array becomes:  
     `[444, 13, 390, 432, 115, 195, 107, 106, 28, 102]`  
   - Heapify index 1:  
     - Children: index 3 → 432, index 4 → 115.  
     - Largest is 432 → swap.  
     - Array becomes:  
       `[444, 432, 390, 13, 115, 195, 107, 106, 28, 102]`  
     - Heapify index 3:  
       - Children: index 7 → 106, index 8 → 28.  
       - Largest is 106 → swap.  
       - Array becomes:  
         `[444, 432, 390, 106, 115, 195, 107, 13, 28, 102]`

**Max Heap Result:**  
`[444, 432, 390, 106, 115, 195, 107, 13, 28, 102]`

---

**Step 2: Sorting the Heap**

Perform repeated steps of swapping the root with the last element and heapifying:

1. **Iteration 1:**  
   - Swap index 0 (444) with index 9 (102):  
     Array becomes: `[102, 432, 390, 106, 115, 195, 107, 13, 28, 444]`  
   - Reduce heap size to 9 and heapify from index 0 → new max becomes 432 moves to root.

2. **Iteration 2:**  
   - Swap new root with element at index 8.  
   - Continue heapifying.

3. **Repeat:**  
   Continue this process until the heap is reduced to size 1.

**Final Sorted Array (Ascending Order):**  
[13, 28, 102, 106, 107, 115, 195, 390, 432, 444]

*Note:* (A detailed step-by-step swap and heapify process is performed for each iteration; the final outcome is as shown.)

---

## Q9. Short Notes on Selected Topics

### (a) Circular Queue

- **Definition:**  
  A circular queue is a linear data structure that uses a fixed-size array in a circular fashion. The end of the array wraps around to the beginning, which helps in utilizing the array space efficiently.

- **Key Characteristics:**  
  - Uses two indices (front and rear) to keep track of elements.  
  - The condition for “full” is typically when the next position of rear equals the front.  
  - It eliminates the “waste” of unused space in a normal linear queue when elements are dequeued.

- **Example Application:**  
  - Used in CPU scheduling and buffering in streaming data.

---

### (b) Depth First Search (DFS)

- **Definition:**  
  DFS is a graph traversal algorithm that starts at a selected node (source) and explores as far as possible along each branch before backtracking.

- **Key Characteristics:**  
  - Can be implemented using recursion or a stack.  
  - Explores a branch completely before moving to another branch.  
  - Useful for solving puzzles, maze traversal, and checking connectivity in graphs.

- **Example:**  
  - In a maze, DFS will follow one path all the way to its end before trying alternate paths.

---

### (c) B Tree

- **Definition:**  
  A B Tree is a self-balancing multi-way tree data structure that maintains sorted data and allows searches, sequential access, insertions, and deletions in logarithmic time.

- **Key Characteristics:**  
  - Designed to work well on storage devices (like disks).  
  - Nodes can have more than two children (a B tree of order _m_ can have up to _m_ children).  
  - Ensures that all leaves are at the same depth.

- **Example Application:**  
  - Widely used in database indexing and file systems.

---

### (d) Adjacency Matrix

- **Definition:**  
  An adjacency matrix is a 2D array used to represent a graph. The rows and columns correspond to vertices; an entry at (i, j) indicates whether there is an edge from vertex i to vertex j (and may store edge weights).

- **Key Characteristics:**  
  - Provides O(1) time complexity for edge lookup.  
  - Can be space-inefficient for sparse graphs because it uses O(V²) space.
  
- **Example:**  
  - Used when the graph is dense or when quick edge existence checks are required.

---

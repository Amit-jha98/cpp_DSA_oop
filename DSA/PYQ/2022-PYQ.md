### PYQ-2022

---

### Q1 (a)
**Question:**  
In a stack, if a user tries to remove an element from an empty stack it is called:

**Options:**  
(i) underflow  
(ii) empty collection  
(iii) garbage collection  
(iv) overflow

**Answer:** (i) underflow

**Explanation:**  
When you attempt to remove (pop) an element from an empty stack, the condition is known as an "underflow." This occurs because there is no element available to remove from the stack. In contrast, "overflow" happens when attempting to push an element onto a full stack.

---

### Q1 (b)
**Question:**  
Consider the binary max-heap implemented using an array. Which one of the following arrays represents the heap?

**Options:**  
(i) 25, 12, 16, 13, 10, 8, 14  
(ii) 25, 12, 16, 13, 10, 8, 14  
(iii) 25, 14, 16, 13, 10, 8, 12  
(iv) 25, 14, 12, 13, 10, 8, 16

**Answer:** (iii) 25, 14, 16, 13, 10, 8, 12

**Explanation:**  
A binary max-heap must satisfy the heap property: each parent node is greater than or equal to its children.  
- In (iii):  
  - **Index 0 (25):** Children at indices 1 (14) and 2 (16) → 25 ≥ 14 and 25 ≥ 16.  
  - **Index 1 (14):** Children at indices 3 (13) and 4 (10) → 14 ≥ 13 and 14 ≥ 10.  
  - **Index 2 (16):** Children at indices 5 (8) and 6 (12) → 16 ≥ 8 and 16 ≥ 12.  
Thus, option (iii) satisfies the max-heap condition.

---

### Q1 (c)
**Question:**  
A hash function defined as h(key) = key mod 7 is used with linear probing to insert keys 44, 45, 79, 55, 91, 18, 63 into a table indexed from 0 to 6. What will be the location of key 18?

**Options:**  
(i) 3  
(ii) 4  
(iii) 5  
(iv) 6

**Answer:** (iii) 5

**Explanation:**  
Perform the insertion step by step using h(key)= key mod 7 and linear probing:  
1. **44:** 44 mod 7 = 2 → placed at index 2.  
2. **45:** 45 mod 7 = 3 → placed at index 3.  
3. **79:** 79 mod 7 = 2 → index 2 is occupied; probe to index 3 (occupied); then index 4 is free → placed at index 4.  
4. **55:** 55 mod 7 = 6 → placed at index 6.  
5. **91:** 91 mod 7 = 0 → placed at index 0.  
6. **18:** 18 mod 7 = 4 → index 4 is occupied (by 79); probe next index, index 5 is free → placed at index 5.  
7. **63:** 63 mod 7 = 0 → index 0 is occupied (by 91); probe to index 1 which is free → placed at index 1.  
Thus, key 18 is placed at index **5**.

---

### Q1 (d)
**Question:**  
If the number of values to be sorted is already partially sorted, then ______ sorting can be efficient.

**Options:**  
(i) merge  
(ii) insertion  
(iii) bubble  
(iv) selection

**Answer:** (ii) insertion

**Explanation:**  
Insertion sort is particularly efficient for lists that are already partially sorted. Its best-case running time is O(n) when the input is nearly sorted, because each element is moved only a few positions.

---

### Q1 (e)
**Question:**  
The time complexity of merge sort is:

**Options:**  
(i) O(n)  
(ii) O(log n)  
(iii) O(n log n)  
(iv) O(n²)

**Answer:** (iii) O(n log n)

**Explanation:**  
Merge sort is a divide-and-conquer algorithm that splits the array into halves (which takes O(log n) divisions) and then merges the sorted halves in O(n) time. Therefore, the overall time complexity is O(n log n).

---

### Q1 (f)
**Question:**  
State true or false:  
A: Binary search is used for searching in a sorted array.  
B: The time complexity of binary search is O(log n)

**Options:**  
(i) True, False  
(ii) False, True  
(iii) False, False  
(iv) True, True

**Answer:** (iv) True, True

**Explanation:**  
Both statements are correct. Binary search is applicable only to sorted arrays, and its time complexity is O(log n) due to the halving of the search space with each comparison.

---

### Q1 (g)
**Question:**  
In a circular linked list organization, insertion of a record involves modification of:

**Options:**  
(i) One pointer  
(ii) Two pointers  
(iii) More than two pointers  
(iv) No pointer

**Answer:** (ii) Two pointers

**Explanation:**  
When inserting a node in a circular linked list, you typically adjust two pointers. For example, if inserting after a given node, you set the new node’s next pointer to the given node’s next, and then update the given node’s next pointer to point to the new node.

---

### Q1 (h)
**Question:**  
Level order traversal of a rooted tree can be done by starting from the root and performing:

**Options:**  
(i) pre-order traversal  
(ii) in-order traversal  
(iii) depth-first search  
(iv) breadth-first search

**Answer:** (iv) breadth-first search

**Explanation:**  
Level order traversal visits nodes level by level from the root downward. This is equivalent to a breadth-first search (BFS), which uses a queue to track nodes at the current level before moving to the next level.

---

### Q1 (i)
**Question:**  
An Abstract Data Type (ADT) is:

**Options:**  
(i) same as an abstract class  
(ii) a data type that cannot be instantiated  
(iii) a data type for which only the operations defined on it can be used, but none else  
(iv) all of the above

**Answer:** (iii) a data type for which only the operations defined on it can be used, but none else

**Explanation:**  
An ADT is a conceptual model that defines the behavior (operations) of a data type without specifying its implementation. It is not necessarily the same as an abstract class in object-oriented programming, nor is it simply a data type that cannot be instantiated.

---

### Q1 (j)
**Question:**  
How many distinct BSTs can be constructed with 3 distinct keys?

**Options:**  
(i) 4  
(ii) 5  
(iii) 6  
(iv) 9

**Answer:** (ii) 5

**Explanation:**  
The number of distinct binary search trees (BSTs) that can be constructed with _n_ distinct keys is given by the nth Catalan number. For n = 3, the number is  
\[
C_3 = \frac{1}{3+1} \binom{6}{3} = \frac{1}{4} \times 20 = 5.
\]

---

---

## Q.2: Asymptotic Notations and Recurrence Relations

### (a) Asymptotic Notations (7 marks)

When comparing the time complexity of algorithms, we use asymptotic notations to describe how the running time grows as a function of the input size (n). The most common notations are:

1. **Big-O (O) – Upper Bound:**  
   - **Definition:** A function _f(n)_ is said to be O(_g(n)_) if there exist positive constants _c_ and _n₀_ such that for all _n ≥ n₀_  
     \[
     f(n) \leq c \cdot g(n)
     \]
   - **Example:** For linear search, the worst-case running time is O(n) since in the worst case every element is examined.
   - **Diagram Illustration:**  
     Imagine plotting _f(n)_ and _g(n)_ on the same graph. Beyond some point _n₀_, the curve of _f(n)_ always lies below a constant multiple of _g(n)_.

2. **Big-Omega (Ω) – Lower Bound:**  
   - **Definition:** _f(n)_ is Ω(_g(n)_) if there exist constants _c_ and _n₀_ such that for all _n ≥ n₀_  
     \[
     f(n) \geq c \cdot g(n)
     \]
   - **Example:** Even in the best case, some algorithms (e.g., any algorithm that must read every input element) have a lower bound of Ω(n).

3. **Theta (Θ) – Tight Bound:**  
   - **Definition:** _f(n)_ is Θ(_g(n)_) if it is both O(_g(n)_) and Ω(_g(n)_); that is, there exist constants _c₁, c₂,_ and _n₀_ such that for all _n ≥ n₀_  
     \[
     c₁ \cdot g(n) \leq f(n) \leq c₂ \cdot g(n)
     \]
   - **Example:** Merge sort always runs in Θ(n log n) time. Both the upper and lower bounds are of the order n log n.
   - **Figure (Conceptual):**  
     Imagine two curves—one representing _f(n)_ and another representing _g(n)_. If they grow at the same rate (differing only by constant factors), then _f(n)_ is Θ(_g(n)_).

These notations let us ignore constant factors and lower order terms so that we can compare algorithms in a machine‐independent way.

---

### (b) Solving the Recurrence \( T(n) = 2T(n/2) + n \) (7 marks)

We are given the recurrence relation  
\[
T(n) = 2T\left(\frac{n}{2}\right) + n, \quad n \ge 2 \quad \text{with } T(1)=0.
\]

**Step 1: Identify parameters for the Master Theorem.**

The recurrence is of the form:  
\[
T(n) = a \, T\left(\frac{n}{b}\right) + f(n)
\]
Here:  
- \(a = 2\)  
- \(b = 2\)  
- \(f(n) = n\)

**Step 2: Compare \( f(n) \) with \( n^{\log_b a} \).**

Calculate:  
\[
\log_b a = \log_2 2 = 1 \quad \Longrightarrow \quad n^{\log_b a} = n^1 = n.
\]
Since \( f(n) \) is Θ(n), we are in the case where  
\[
f(n) = \Theta(n^{\log_b a})
\]

**Step 3: Apply the Master Theorem conclusion.**

For this case, the Master Theorem tells us that:  
\[
T(n) = \Theta(n \log n).
\]

**Final Answer:** The time complexity is **Θ(n log n)**.

---

## Q.3: Tree Traversals and BST Construction

### (a) Tree Traversal Techniques and Non-Recursive Pre-Order (7 marks)

**Tree Traversal Techniques:**

- **Pre-order Traversal:**  
  Visit **root**, then **left subtree**, then **right subtree**.  
  *Example Order:* For a tree with root A and children B and C, pre-order yields: A, B, C.

- **In-order Traversal:**  
  Visit **left subtree**, then **root**, then **right subtree**.  
  *Example Order:* In a binary search tree (BST), in-order traversal outputs elements in sorted order.

- **Post-order Traversal:**  
  Visit **left subtree**, then **right subtree**, then **root**.  
  *Example Order:* Useful for deleting trees or evaluating expression trees.

**Non-Recursive Pre-Order Traversal in C:**

To perform a pre-order traversal without recursion, we use a stack. Below is an example C function:

```c
#include <stdio.h>
#include <stdlib.h>

// Definition for a tree node.
typedef struct Node {
    int data;
    struct Node* left;
    struct Node* right;
} Node;

// Stack node for pointers to tree nodes.
typedef struct Stack {
    Node* data;
    struct Stack* next;
} Stack;

// Function to push a node onto the stack.
void push(Stack** top, Node* node) {
    Stack* newNode = (Stack*)malloc(sizeof(Stack));
    newNode->data = node;
    newNode->next = *top;
    *top = newNode;
}

// Function to pop a node from the stack.
Node* pop(Stack** top) {
    if (*top == NULL)
        return NULL;
    Stack* temp = *top;
    Node* res = temp->data;
    *top = temp->next;
    free(temp);
    return res;
}

// Non-recursive pre-order traversal.
void preOrderNonRecursive(Node* root) {
    if (root == NULL)
        return;
    
    Stack* stack = NULL;
    push(&stack, root);
    
    while (stack != NULL) {
        Node* current = pop(&stack);
        // Process the current node.
        printf("%d ", current->data);
        
        // Push right child first so that left is processed first.
        if (current->right)
            push(&stack, current->right);
        if (current->left)
            push(&stack, current->left);
    }
}
```

**Explanation:**  
We start by pushing the root onto the stack. Then, while the stack is not empty, we pop the top node, print its data, and push its right child (if any) followed by its left child (if any) so that the left child is processed next. This effectively mimics the recursive pre-order traversal.

---

### (b) Constructing a BST from Pre-order and Deriving Post-order (7 marks)

**Given Pre-order Sequence:**  
30, 20, 10, 15, 25, 31, 39, 35, 42

**Step 1: Construct the BST**

- **Root:** 30 (first element).
- **Left Subtree:** All elements less than 30: 20, 10, 15, 25.
  - **Root of left subtree:** 20.
  - For subtree of 20, take next elements:
    - 10 is less than 20 → goes to left of 20.
    - 15 is greater than 10 but less than 20 → becomes right child of 10.
    - 25 is greater than 20 → goes to right of 20.
  
  Left subtree structure:
  ```
         20
        /  \
      10    25
        \
         15
  ```

- **Right Subtree:** All elements greater than 30: 31, 39, 35, 42.
  - **Root of right subtree:** 31.
  - Remaining elements: 39, 35, 42.
    - For subtree with root 39:
      - 35 < 39 → goes to left of 39.
      - 42 > 39 → goes to right of 39.
  
  Right subtree structure:
  ```
      31
        \
         39
        /  \
      35    42
  ```

- **Final BST:**
  ```
              30
             /   \
           20     31
          /  \       \
        10    25      39
          \
           15      /     \
                 35       42
  ```

**Step 2: Derive the Post-order Traversal**

Post-order traversal order: **Left subtree → Right subtree → Root.**

- **Left Subtree (rooted at 20):**  
  - For node 10: Left is NULL; right is 15 → post-order of 10 is: 15, then 10.
  - For node 20: Process left (15, 10), then right (25), then node 20.
  
  Left subtree post-order: **15, 10, 25, 20**

- **Right Subtree (rooted at 31):**  
  - For node 39: Process left (35), then right (42), then 39.
  - For node 31: Left is NULL; then process right subtree (35, 42, 39), then node 31.
  
  Right subtree post-order: **35, 42, 39, 31**

- **Full Tree Post-order:**  
  Combine left subtree post-order, right subtree post-order, then root (30):  
  **15, 10, 25, 20, 35, 42, 39, 31, 30**

---

## Q.4: Circular Queue and Infix-to-Postfix Conversion

### (a) Circular Queue Insertion and Deletion (7 marks)

Below are C functions implementing insertion (enqueue) and deletion (dequeue) operations on a circular queue using an array.

```c
#include <stdio.h>
#include <stdlib.h>
#define SIZE 10  // Example size; can be set to any n

typedef struct {
    int items[SIZE];
    int front, rear;
} CircularQueue;

// Initialize the queue
void initQueue(CircularQueue *q) {
    q->front = q->rear = 0;
}

// Check if the queue is empty
int isEmpty(CircularQueue *q) {
    return (q->front == q->rear);
}

// Check if the queue is full (one slot is unused)
int isFull(CircularQueue *q) {
    return ((q->rear + 1) % SIZE == q->front);
}

// Insertion (Enqueue)
void enqueue(CircularQueue *q, int value) {
    if (isFull(q)) {
        printf("Queue Overflow\n");
        return;
    }
    q->rear = (q->rear + 1) % SIZE;
    q->items[q->rear] = value;
}

// Deletion (Dequeue)
int dequeue(CircularQueue *q) {
    if (isEmpty(q)) {
        printf("Queue Underflow\n");
        return -1;  // or some error code
    }
    q->front = (q->front + 1) % SIZE;
    return q->items[q->front];
}
```

**Explanation:**  
- The queue uses two indices, `front` and `rear`.  
- **Empty condition:** When `front == rear`.  
- **Full condition:** When `(rear + 1) % SIZE == front`.  
- The enqueue function advances the rear pointer and inserts the element.  
- The dequeue function advances the front pointer and returns the element.

---

### (b) Infix to Postfix Conversion (7 marks)

Convert the expression:  
\[
A + B / C - (D + E) - F
\]
using a stack.

**Algorithm:**  
- **Operands:** Append to output.
- **Operators:** Push on stack; pop if top has higher or equal precedence.
- **Left Parenthesis "(" :** Push onto stack.
- **Right Parenthesis ")" :** Pop from stack until "(" is encountered.

**Step-by-Step Conversion:**

| **Symbol** | **Action**                                                                                                           | **Stack**         | **Output**     |
|------------|----------------------------------------------------------------------------------------------------------------------|-------------------|----------------|
| A          | Operand: output it                                                                                                   | (empty)           | A              |
| +          | Operator: stack is empty → push '+'                                                                                  | +                 | A              |
| B          | Operand: output it                                                                                                   | +                 | A B            |
| /          | Operator: '/' has higher precedence than '+' → push '/'                                                               | +, /              | A B            |
| C          | Operand: output it                                                                                                   | +, /              | A B C          |
| -          | Operator: now, '-' is encountered. Pop until stack is empty or an operator with lower precedence is found.            | +, /              | A B C          |
|            | Pop '/' (since '/' > '-' in precedence) → output it.                                                                 | +                 | A B C /        |
|            | Next, top is '+' which has the same precedence as '-' (for left-associative operators, pop equal precedence) → pop '+' and output it. | (empty)           | A B C / +      |
|            | Push '-' onto stack.                                                                                                 | -                 | A B C / +      |
| (          | Left parenthesis: push '('                                                                                           | -, (              | A B C / +      |
| D          | Operand: output it                                                                                                   | -, (              | A B C / + D    |
| +          | Operator: stack top is '(' so push '+'                                                                             | -, (, +           | A B C / + D    |
| E          | Operand: output it                                                                                                   | -, (, +           | A B C / + D E  |
| )          | Right parenthesis: pop until '(' is encountered. Pop '+' and output it, then pop '(' (discard it).                    | -                 | A B C / + D E +|
| -          | Operator: '-' encountered. Compare with top of stack which is '-' (equal precedence) → pop '-' and output it, then push '-' | (empty), then push '-' | A B C / + D E + - |
| F          | Operand: output it                                                                                                   | -                 | A B C / + D E + - F |
| End        | End of expression: pop remaining operator(s) from stack.                                                             | (empty)           | A B C / + D E + - F - |

**Final Postfix Expression:**  
```
A B C / + D E + - F -
```

---

## Q.5: Linked List Deletion and Heap Insertion

### (a) Delete Last Node from Singly Linked List (7 marks)

**C Function to Delete the Last Node:**

```c
#include <stdio.h>
#include <stdlib.h>

typedef struct Node {
    int data;
    struct Node* next;
} Node;

// Function to delete the last node from the list.
void deleteLastNode(Node **head_ref) {
    // If the list is empty.
    if (*head_ref == NULL) {
        printf("List is empty\n");
        return;
    }
    
    // If there is only one node.
    if ((*head_ref)->next == NULL) {
        free(*head_ref);
        *head_ref = NULL;
        return;
    }
    
    // Traverse to the second last node.
    Node *temp = *head_ref;
    while (temp->next->next != NULL) {
        temp = temp->next;
    }
    
    // temp->next is the last node.
    free(temp->next);
    temp->next = NULL;
}
```

**Explanation:**  
- Check if the list is empty.  
- If only one node exists, free it and set the head to NULL.  
- Otherwise, traverse until the second last node is reached, then free the last node and update the second last node’s `next` pointer to NULL.

---

### (b) Max-Heap Insertion (7 marks)

We need to build a max-heap by inserting keys in the order:  
25, 35, 18, 9, 46, 70, 48

We represent the heap as an array and show the adjustments (heapify up) after each insertion.

1. **Insert 25:**  
   - Heap: [25]

2. **Insert 35:**  
   - Place at next position: [25, 35]  
   - Compare 35 with parent (25): 35 > 25 → swap  
   - Heap becomes: [35, 25]

3. **Insert 18:**  
   - Insert at index 2: [35, 25, 18]  
   - Parent of index 2 is 35. Since 18 < 35, no swap.

4. **Insert 9:**  
   - Insert at index 3: [35, 25, 18, 9]  
   - Parent (index 1, value 25): 9 < 25 → no swap.

5. **Insert 46:**  
   - Insert at index 4: [35, 25, 18, 9, 46]  
   - Parent of index 4 (index 1, value 25): 46 > 25 → swap  
   - Heap becomes: [35, 46, 18, 9, 25]  
   - Now, new position of 46 is at index 1; parent (index 0, value 35): 46 > 35 → swap  
   - Heap becomes: [46, 35, 18, 9, 25]

6. **Insert 70:**  
   - Insert at index 5: [46, 35, 18, 9, 25, 70]  
   - Parent of index 5 (index 2, value 18): 70 > 18 → swap  
   - Heap becomes: [46, 35, 70, 9, 25, 18]  
   - New position of 70 is index 2; parent (index 0, value 46): 70 > 46 → swap  
   - Heap becomes: [70, 35, 46, 9, 25, 18]

7. **Insert 48:**  
   - Insert at index 6: [70, 35, 46, 9, 25, 18, 48]  
   - Parent of index 6 (index 2, value 46): 48 > 46 → swap  
   - Heap becomes: [70, 35, 48, 9, 25, 18, 46]  
   - Now, at index 2, parent is index 0 (70) and 48 < 70, so no further swap.

**Final Max-Heap Array:**  
```
[70, 35, 48, 9, 25, 18, 46]
```

**Illustration at each step clearly shows the “heapify-up” process that maintains the max-heap property.**

---

## Q.6: Merge Sort and Collision Resolution in Hashing

### (a) Merge Sort Algorithm, Time and Space Complexity (7 marks)

**Merge Sort Algorithm (Pseudocode):**

1. **Divide:**  
   - If the array has one element, it is already sorted.
   - Otherwise, divide the array into two halves.

2. **Conquer:**  
   - Recursively apply merge sort to both halves.

3. **Combine (Merge):**  
   - Merge the two sorted halves into a single sorted array.

```pseudo
function mergeSort(array, left, right):
    if left < right:
        mid = (left + right) / 2
        mergeSort(array, left, mid)
        mergeSort(array, mid+1, right)
        merge(array, left, mid, right)
```

**Merge Function Pseudocode:**

```pseudo
function merge(array, left, mid, right):
    n1 = mid - left + 1
    n2 = right - mid
    create temporary arrays L[1..n1] and R[1..n2]
    copy array[left..mid] into L[]
    copy array[mid+1..right] into R[]
    i = 0; j = 0; k = left
    while i < n1 and j < n2:
        if L[i] <= R[j]:
            array[k] = L[i]
            i = i + 1
        else:
            array[k] = R[j]
            j = j + 1
        k = k + 1
    copy any remaining elements of L[] into array
    copy any remaining elements of R[] into array
```

**Time Complexity:**  
- Each merge operation takes O(n) time.  
- The array is divided log₂n times.  
- **Overall:** Θ(n log n)

**Space Complexity:**  
- Requires extra space for temporary arrays during merge.  
- **Overall:** O(n)

---

### (b) Collision in Hashing and Resolution Techniques (7 marks)

**Collision:**  
A collision occurs when two distinct keys hash to the same index in a hash table.

**Methodologies to Resolve Collisions:**

1. **Open Addressing:**  
   - All elements are stored in the hash table itself.
   - When a collision occurs, the algorithm searches for the next free slot.
   - **Techniques:**
     - **Linear Probing:** Check the next slot sequentially.
     - **Quadratic Probing:** Use a quadratic function to find the next slot.
     - **Double Hashing:** Use a second hash function to compute the offset.

2. **Separate Chaining (Chaining):**  
   - Each slot in the hash table contains a pointer to a linked list (or other data structure).
   - All keys that hash to the same index are stored in the linked list at that index.
   - **Advantage:**  
     - Easier to implement and can handle a large number of collisions gracefully.
   - **Disadvantage:**  
     - Extra memory overhead due to pointers and linked list management.

---

## Q.7: Counting Leaf Nodes and Graph Traversals

### (a) Count Leaf Nodes in a Binary Tree (7 marks)

**Algorithm to Count Leaf Nodes (Pseudocode):**

```pseudo
function countLeafNodes(root):
    if root is NULL:
        return 0
    if root.left is NULL and root.right is NULL:
        return 1
    return countLeafNodes(root.left) + countLeafNodes(root.right)
```

**Explanation:**  
- If the tree is empty, return 0.
- If a node has no children (both left and right are NULL), it is a leaf; count it as 1.
- Otherwise, recursively count the leaf nodes in the left and right subtrees.
- **Complexity:**  
  - Time Complexity: O(n) (each node is visited once)  
  - Space Complexity: O(h) (recursion stack; h = tree height)

---

### (b) BFS vs. DFS and BFS Algorithm (7 marks)

**Comparison:**

- **Breadth-First Search (BFS):**  
  - Explores the graph level by level (all neighbors before going deeper).  
  - Uses a queue to maintain the order of nodes.  
  - **Time Complexity:** O(V + E)  
  - **Space Complexity:** O(V)

- **Depth-First Search (DFS):**  
  - Explores as far as possible along a branch before backtracking.  
  - Uses a stack (or recursion) to track nodes.  
  - **Time Complexity:** O(V + E)  
  - **Space Complexity:** O(V) in the worst case.

**BFS Algorithm (Pseudocode):**

```pseudo
function BFS(Graph, start):
    create an empty queue Q
    mark start as visited and enqueue start into Q
    while Q is not empty:
        current = dequeue(Q)
        process current (e.g., print current)
        for each neighbor v of current:
            if v is not visited:
                mark v as visited
                enqueue v into Q
```

**Explanation:**  
- Start from the initial node, mark it as visited, and use a queue to process each level.
- Dequeue a node, process it, and enqueue its unvisited neighbors.
- Continue until the queue is empty.

---

## Q.8: Data Types and Doubly Linked Lists

### (a) System-Defined Data Types vs. Abstract Data Types (7 marks)

**System-Defined Data Types:**
- These are built-in types provided by programming languages.
- **Examples:** `int`, `float`, `char`, `double`, etc.
- They come with predefined operations and memory layouts.

**Abstract Data Types (ADT):**
- An ADT defines a data type by its behavior (operations) rather than its implementation.
- **Examples:** Stack, Queue, List, Tree.
- The implementation details are hidden from the user; only the operations (like push, pop, enqueue, dequeue) are exposed.
- **Advantage:**  
  - Promotes modularity and encapsulation.
  - The user interacts with the data type only through its defined interface.

---

### (b) Doubly Linked List and Adding a Node at the End (7 marks)

**Doubly Linked List:**
- A doubly linked list is a linked list in which each node contains two pointers: one pointing to the previous node and one to the next node.
- **Applications:**  
  - Browser history navigation, undo functionality in editors, and managing playlists.
- **Pseudo Code for Insertion at the End:**

```pseudo
function insertAtEnd(head, value):
    newNode = createNode(value)
    newNode.next = NULL
    newNode.prev = NULL
    
    // If list is empty, newNode becomes head.
    if head == NULL:
        head = newNode
        return head
    
    // Traverse to the last node.
    current = head
    while current.next != NULL:
        current = current.next
    
    // Insert newNode after the last node.
    current.next = newNode
    newNode.prev = current
    return head
```

**Explanation:**  
- A new node is created and initialized.
- If the list is empty, the new node becomes the head.
- Otherwise, traverse to the end of the list and update the pointers:  
  - The last node’s `next` pointer is set to the new node.
  - The new node’s `prev` pointer is set to the last node.

---

## Q.9: Short Notes (Choose Two Topics) (2 × 7 marks)

Here we present short notes on two topics:

### (a) AVL Rotations

**AVL Trees:**
- An AVL tree is a self-balancing binary search tree where the heights of the left and right subtrees differ by at most one.
- **When imbalance occurs, rotations are used to restore balance.**

**Types of AVL Rotations:**
1. **Single Rotation:**
   - **Left Rotation:** Used when a node’s right subtree is heavier.
   - **Right Rotation:** Used when a node’s left subtree is heavier.
2. **Double Rotation:**
   - **Left-Right Rotation:** A left rotation on the left child followed by a right rotation on the node.
   - **Right-Left Rotation:** A right rotation on the right child followed by a left rotation on the node.

**Illustration:**  
- For a Right-Right imbalance, perform a single left rotation.  
- For a Left-Right imbalance, perform a left rotation on the left child and then a right rotation on the node.

**Purpose:**  
Rotations help maintain the AVL balance factor (difference in heights ≤ 1) ensuring O(log n) time for insertion, deletion, and search operations.

---

### (b) Open Addressing & Chaining in Hashing

**Collision Resolution in Hashing:**

When two keys hash to the same index, a collision occurs. Two primary methods to resolve collisions are:

1. **Open Addressing:**
   - All keys are stored within the hash table.
   - **Techniques:**
     - **Linear Probing:** If a collision occurs, sequentially check the next available slot.
     - **Quadratic Probing:** Use a quadratic function (e.g., i²) to determine the interval between slots.
     - **Double Hashing:** Use a second hash function to compute the step size.
   - **Advantage:**  
     - Simple implementation; no extra data structures are needed.
   - **Disadvantage:**  
     - Can lead to clustering and performance degradation when load factors are high.

2. **Separate Chaining (Chaining):**
   - Each index of the hash table contains a pointer to a linked list (or other container) of entries that hash to the same index.
   - **Advantage:**  
     - Handles collisions gracefully and can accommodate an arbitrary number of keys per slot.
   - **Disadvantage:**  
     - Requires extra memory for pointers and linked list nodes.

---

